import os
import sys
import argparse
from datetime import datetime

os.environ["VLLM_WORKER_MULTIPROC_METHOD"] = "spawn"  # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹å¼ï¼Œé˜²æ­¢ vLLM å†²çª

flashrag_path = '/data/wyh/MedicalRAG/FlashRAG'
sys.path.insert(0, flashrag_path)


from flashrag.config import Config
from flashrag.utils import get_dataset
from flashrag.pipeline import SequentialPipeline
from flashrag.prompt import PromptTemplate

# ================= 1. Prompt å®šä¹‰åŒºåŸŸ =================

TEST_PROMPT = """
Answer the following question.
{reference}
"""

# Baseline (æ—  RAG)
BASELINE_SYSTEM_PROMPT = """
Answer the following question. You should only use your own knowledge.
"""

# Strict RAG (ä¸¥æ ¼æ£€ç´¢)
STRICT_SYSTEM_PROMPT = """
Answer the question based ONLY on the following Reference Q&A.
If the reference does not contain the answer, say "I don't know".

Reference Q&A:
{reference}
"""

# Hybrid RAG (æ··åˆç­–ç•¥)
HYBRID_SYSTEM_PROMPT = """
Answer the question using your own knowledge and the following Reference Q&A.

Reference Q&A:
{reference}
"""

# ç­–ç•¥å­—å…¸æ˜ å°„
PROMPT_MAP = {
    "test": TEST_PROMPT,
    "baseline": BASELINE_SYSTEM_PROMPT,
    "strict": STRICT_SYSTEM_PROMPT,
    "hybrid": HYBRID_SYSTEM_PROMPT
}

# ================= 2. ä¸»ç¨‹åºé€»è¾‘ =================

def main(args):
    # 1. ç¡®å®š Prompt å’Œ ä¿å­˜è·¯å¾„
    system_prompt = PROMPT_MAP[args.strategy]
    
    # ç»“æœä¿å­˜è·¯å¾„ï¼šoutput/{strategy}/
    base_save_dir = "/data/wyh/MedicalRAG/output"
    current_save_dir = os.path.join(base_save_dir, args.strategy)
    os.makedirs(current_save_dir, exist_ok=True)

    print(f"\n{'='*40}")
    print(f"ğŸš€ æ­£åœ¨å¯åŠ¨å®éªŒ: {args.strategy.upper()}")
    print(f"ğŸ“‚ ç»“æœä¿å­˜è·¯å¾„: {current_save_dir}")
    print(f"{'='*40}\n")

    # 2. æ„å»ºé…ç½®å­—å…¸
    config_dict = {
        "data_dir": "/data/wyh/MedicalRAG/data",
        "dataset_name": "Huatuo26M-Lite",
        "split": args.split,

        # æ¡†æ¶ä¸è¯„æµ‹
        "framework": "host",  # 'host', 'api', 'vllm'
        # "generator_model": "qwen2.5-7B-instruct",
        "generator_batch_size": args.batch_size,
        "generation_params": {
            "max_tokens": 512,
            "temperature": 0.1, # åŒ»å­¦é—®é¢˜ä¿æŒä½éšæœºæ€§
            "top_p": 0.9
        },
        # "metrics": ['acc', 'em', 'f1', 'bleu', 'rouge-l', 'recall', 'precision', 'rouge-1', 'rouge-2'],
        "metrics": ['gpt_harmful_rate', 'gpt_hallucination_rate'],
        
        "api_setting": {
            "model_name": "gpt-4o-mini",
            "generator_model": "deepseek-r1",
            "concurrency": args.batch_size, # API å¹¶å‘æ•°
            "timeout_sec": 60,
            # "api_key": os.getenv("OPENAI_API_KEY")
        },

        # æ£€ç´¢é…ç½®
        # "index_path": "/data/wyh/MedicalRAG/data/indexes/huatuo_bm25_index/bm25",
        "index_path": "/data/wyh/MedicalRAG/data/indexes/huatuo_bge_index/bge_Flat.index",
        "corpus_path": "/data/wyh/MedicalRAG/data/indexes/corpus.jsonl",
        "retrieval_method": "bge",
        "retrieval_topk": 5,
        # "bm25_backend": "bm25s", # if and only if retrieval_method == bm25

        # ç¡¬ä»¶é…ç½®
        "gpu_id": args.gpu_id,
        "gpu_num": len(args.gpu_id.split(',')),
        "gpu_memory_utilization": 0.8,
        
        # ä¿å­˜è·¯å¾„
        "save_dir": current_save_dir
    }

    # 3. åˆå§‹åŒ– Config å’Œ Dataset
    config = Config("my_config.yaml", config_dict=config_dict)
    all_split = get_dataset(config)
    test_data = all_split[args.split]
    
    # Prompt
    template = PromptTemplate(
        config,
        system_prompt=system_prompt,
        user_prompt="""Question: {question}"""
    )

    # Pipeline
    pipeline = SequentialPipeline(config, template)

    if args.strategy == "baseline" or args.strategy == "test":
        print(">>> æ­£åœ¨æ‰§è¡Œ Baseline æ¨¡å¼ (Naive Run - æ— æ£€ç´¢)...")
        result = pipeline.naive_run(test_data, do_eval=True)
    else:
        print(f">>> æ­£åœ¨æ‰§è¡Œ {args.strategy} RAG æ¨¡å¼ (Run - å«æ£€ç´¢)...")
        result = pipeline.run(test_data, do_eval=True)

    print(f"\nâœ… å®éªŒç»“æŸï¼ç»“æœå·²ä¿å­˜è‡³: {current_save_dir}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Medical RAG Experiment Runner")
    parser.add_argument("--strategy", type=str, required=True, 
                        choices=["baseline", "strict", "hybrid", "test"],
                        help="é€‰æ‹©å®éªŒç­–ç•¥: baseline(æ— RAG), strict(ä¸¥æ ¼RAG), hybrid(æ··åˆRAG), test(æµ‹è¯•)")
    parser.add_argument("--gpu_id", type=str, default="0, 1", help="ä½¿ç”¨çš„ GPU IDï¼Œä¾‹å¦‚ '0,1'")
    parser.add_argument("--split", type=str, default="test", help="æµ‹è¯•é›†åˆ‡åˆ†åç§°")
    parser.add_argument("--batch_size", type=int, default=64, help="æ¨ç† Batch Size")
    
    args = parser.parse_args()

    main(args)
